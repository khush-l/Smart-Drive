{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 route options.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 108\u001b[0m\n\u001b[1;32m    105\u001b[0m destination \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHouston, TX\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;66;03m# Generate features and labels for training\u001b[39;00m\n\u001b[0;32m--> 108\u001b[0m route_data \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_route_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43morigin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdestination\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mapi_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;66;03m# Training the Model\u001b[39;00m\n\u001b[1;32m    111\u001b[0m X \u001b[38;5;241m=\u001b[39m route_data[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mroute_length\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mproximity_to_hotspot\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mroute_duration\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n",
      "Cell \u001b[0;32mIn[10], line 88\u001b[0m, in \u001b[0;36mgenerate_route_features\u001b[0;34m(origin, destination, api_key)\u001b[0m\n\u001b[1;32m     86\u001b[0m lat \u001b[38;5;241m=\u001b[39m step[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mend_location\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlat\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     87\u001b[0m lng \u001b[38;5;241m=\u001b[39m step[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mend_location\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlng\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m---> 88\u001b[0m crash_severity \u001b[38;5;241m=\u001b[39m \u001b[43mfind_nearest_crash_severity\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlng\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m crash_severity \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     91\u001b[0m     crash_severity_list\u001b[38;5;241m.\u001b[39mappend(crash_severity)\n",
      "Cell \u001b[0;32mIn[10], line 56\u001b[0m, in \u001b[0;36mfind_nearest_crash_severity\u001b[0;34m(lat, lng, data, max_distance_km)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _, row \u001b[38;5;129;01min\u001b[39;00m data\u001b[38;5;241m.\u001b[39miterrows():\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;66;03m# Calculate the distance between route step and crash data point\u001b[39;00m\n\u001b[1;32m     55\u001b[0m     crash_point \u001b[38;5;241m=\u001b[39m (row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlatitude\u001b[39m\u001b[38;5;124m'\u001b[39m], row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlongitude\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m---> 56\u001b[0m     distance \u001b[38;5;241m=\u001b[39m \u001b[43mgeodesic\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlng\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcrash_point\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mkm\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m distance \u001b[38;5;241m<\u001b[39m min_distance \u001b[38;5;129;01mand\u001b[39;00m distance \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m max_distance_km:\n\u001b[1;32m     59\u001b[0m         min_distance \u001b[38;5;241m=\u001b[39m distance\n",
      "File \u001b[0;32m~/Documents/GitHub/Smart-Drive/venv/lib/python3.11/site-packages/geopy/distance.py:540\u001b[0m, in \u001b[0;36mgeodesic.__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    538\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_ellipsoid(kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mellipsoid\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWGS-84\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m    539\u001b[0m major, minor, f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mELLIPSOID\n\u001b[0;32m--> 540\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/GitHub/Smart-Drive/venv/lib/python3.11/site-packages/geopy/distance.py:276\u001b[0m, in \u001b[0;36mDistance.__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    275\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m a, b \u001b[38;5;129;01min\u001b[39;00m util\u001b[38;5;241m.\u001b[39mpairwise(args):\n\u001b[0;32m--> 276\u001b[0m         kilometers \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmeasure\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    278\u001b[0m kilometers \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m units\u001b[38;5;241m.\u001b[39mkilometers(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    279\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__kilometers \u001b[38;5;241m=\u001b[39m kilometers\n",
      "File \u001b[0;32m~/Documents/GitHub/Smart-Drive/venv/lib/python3.11/site-packages/geopy/distance.py:564\u001b[0m, in \u001b[0;36mgeodesic.measure\u001b[0;34m(self, a, b)\u001b[0m\n\u001b[1;32m    559\u001b[0m lat2, lon2 \u001b[38;5;241m=\u001b[39m b\u001b[38;5;241m.\u001b[39mlatitude, b\u001b[38;5;241m.\u001b[39mlongitude\n\u001b[1;32m    561\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgeod, Geodesic) \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    562\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgeod\u001b[38;5;241m.\u001b[39ma \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mELLIPSOID[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    563\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgeod\u001b[38;5;241m.\u001b[39mf \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mELLIPSOID[\u001b[38;5;241m2\u001b[39m]):\n\u001b[0;32m--> 564\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgeod \u001b[38;5;241m=\u001b[39m \u001b[43mGeodesic\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mELLIPSOID\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mELLIPSOID\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    566\u001b[0m s12 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgeod\u001b[38;5;241m.\u001b[39mInverse(lat1, lon1, lat2, lon2,\n\u001b[1;32m    567\u001b[0m                         Geodesic\u001b[38;5;241m.\u001b[39mDISTANCE)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms12\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    569\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m s12\n",
      "File \u001b[0;32m~/Documents/GitHub/Smart-Drive/venv/lib/python3.11/site-packages/geographiclib/geodesic.py:321\u001b[0m, in \u001b[0;36mGeodesic.__init__\u001b[0;34m(self, a, f)\u001b[0m\n\u001b[1;32m    319\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_A3coeff()\n\u001b[1;32m    320\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_C3coeff()\n\u001b[0;32m--> 321\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C4coeff\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/GitHub/Smart-Drive/venv/lib/python3.11/site-packages/geographiclib/geodesic.py:396\u001b[0m, in \u001b[0;36mGeodesic._C4coeff\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    394\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(Geodesic\u001b[38;5;241m.\u001b[39mnC4_ \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m, l \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m): \u001b[38;5;66;03m# coeff of eps^j\u001b[39;00m\n\u001b[1;32m    395\u001b[0m   m \u001b[38;5;241m=\u001b[39m Geodesic\u001b[38;5;241m.\u001b[39mnC4_ \u001b[38;5;241m-\u001b[39m j \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;66;03m# order of polynomial in n\u001b[39;00m\n\u001b[0;32m--> 396\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_C4x[k] \u001b[38;5;241m=\u001b[39m \u001b[43mMath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolyval\u001b[49m\u001b[43m(\u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcoeff\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_n\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m/\u001b[39m coeff[o \u001b[38;5;241m+\u001b[39m m \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    397\u001b[0m   k \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    398\u001b[0m   o \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m m \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m2\u001b[39m\n",
      "File \u001b[0;32m~/Documents/GitHub/Smart-Drive/venv/lib/python3.11/site-packages/geographiclib/geomath.py:63\u001b[0m, in \u001b[0;36mMath.polyval\u001b[0;34m(N, p, s, x)\u001b[0m\n\u001b[1;32m     59\u001b[0m   \u001b[38;5;66;03m# u + v =       s      + t\u001b[39;00m\n\u001b[1;32m     60\u001b[0m   \u001b[38;5;66;03m#       = round(u + v) + t\u001b[39;00m\n\u001b[1;32m     61\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m s, t\n\u001b[0;32m---> 63\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpolyval\u001b[39m(N, p, s, x):\n\u001b[1;32m     65\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Evaluate a polynomial.\"\"\"\u001b[39;00m\n\u001b[1;32m     67\u001b[0m   y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m N \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m p[s]) \u001b[38;5;66;03m# make sure the returned value is a float\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#old one\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from geopy.distance import geodesic\n",
    "import json\n",
    "import requests\n",
    "import os\n",
    "\n",
    "# Load crash data\n",
    "filename = 'data.csv'\n",
    "data = pd.read_csv(filename, low_memory=False)\n",
    "data = data[['latitude', 'longitude', 'crash_sev_id', 'Crash timestamp (US/Central)']].dropna()\n",
    "\n",
    "# Preprocess crash data (normalize coordinates for clustering)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "coords_scaled = scaler.fit_transform(data[['latitude', 'longitude']].values)\n",
    "\n",
    "# Load the crash zones (GeoJSON) into a format we can work with\n",
    "output_dir = 'output_files'\n",
    "geojson_path = os.path.join(output_dir, 'high_crash_zones.geojson')\n",
    "with open(geojson_path, 'r') as f:\n",
    "    crash_zones = json.load(f)[\"features\"]\n",
    "\n",
    "# Function to calculate proximity to nearest crash hotspot\n",
    "def calculate_proximity_to_hotspot(route):\n",
    "    proximity_scores = []\n",
    "    \n",
    "    for leg in route['legs']:\n",
    "        for step in leg['steps']:\n",
    "            lat = step['end_location']['lat']\n",
    "            lng = step['end_location']['lng']\n",
    "            point = (lat, lng)\n",
    "            \n",
    "            # Calculate distance to each crash hotspot\n",
    "            min_distance = float('inf')\n",
    "            for zone in crash_zones:\n",
    "                zone_polygon = Polygon(zone['geometry']['coordinates'][0])\n",
    "                if zone_polygon.contains(Point(lng, lat)):\n",
    "                    distance = geodesic(point, (zone_polygon.centroid.y, zone_polygon.centroid.x)).km\n",
    "                    min_distance = min(min_distance, distance)\n",
    "            proximity_scores.append(min_distance)\n",
    "\n",
    "    return np.mean(proximity_scores)\n",
    "\n",
    "# Function to find nearest crash data point based on latitude and longitude\n",
    "def find_nearest_crash_severity(lat, lng, data, max_distance_km=1.0):\n",
    "    min_distance = float('inf')\n",
    "    crash_severity = None\n",
    "\n",
    "    for _, row in data.iterrows():\n",
    "        # Calculate the distance between route step and crash data point\n",
    "        crash_point = (row['latitude'], row['longitude'])\n",
    "        distance = geodesic((lat, lng), crash_point).km\n",
    "        \n",
    "        if distance < min_distance and distance <= max_distance_km:\n",
    "            min_distance = distance\n",
    "            crash_severity = row['crash_sev_id']\n",
    "    \n",
    "    # Return the closest severity found within the specified max distance (1 km default)\n",
    "    return crash_severity\n",
    "\n",
    "# Generate features and target variable\n",
    "def generate_route_features(origin, destination, api_key):\n",
    "    routes = get_google_routes(origin, destination, api_key)\n",
    "    \n",
    "    data_points = []\n",
    "    \n",
    "    for route in routes:\n",
    "        # Calculate proximity to crash hotspots\n",
    "        proximity = calculate_proximity_to_hotspot(route)\n",
    "        \n",
    "        # Aggregate features (for simplicity, using only proximity here, but more can be added)\n",
    "        route_features = {\n",
    "            'route_length': sum([step['distance']['value'] for leg in route['legs'] for step in leg['steps']]),\n",
    "            'proximity_to_hotspot': proximity,\n",
    "            'route_duration': sum([step['duration']['value'] for leg in route['legs'] for step in leg['steps']])\n",
    "        }\n",
    "        \n",
    "        # Use crash severity in the route area as the target variable (this needs to be refined further)\n",
    "        crash_severity_list = []\n",
    "        for leg in route['legs']:\n",
    "            for step in leg['steps']:\n",
    "                lat = step['end_location']['lat']\n",
    "                lng = step['end_location']['lng']\n",
    "                crash_severity = find_nearest_crash_severity(lat, lng, data)\n",
    "                \n",
    "                if crash_severity is not None:\n",
    "                    crash_severity_list.append(crash_severity)\n",
    "        \n",
    "        if crash_severity_list:\n",
    "            route_features['average_crash_severity'] = np.mean(crash_severity_list)\n",
    "        else:\n",
    "            route_features['average_crash_severity'] = 0  # No severity data for this route\n",
    "        \n",
    "        data_points.append(route_features)\n",
    "        \n",
    "    return pd.DataFrame(data_points)\n",
    "\n",
    "# Example API key and routes\n",
    "api_key = \"API_Key\"\n",
    "origin = \"Austin, TX\"\n",
    "destination = \"Houston, TX\"\n",
    "\n",
    "# Generate features and labels for training\n",
    "route_data = generate_route_features(origin, destination, api_key)\n",
    "\n",
    "# Training the Model\n",
    "X = route_data[['route_length', 'proximity_to_hotspot', 'route_duration']]\n",
    "y = route_data['average_crash_severity']\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train Random Forest Regressor\n",
    "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f'Mean Squared Error on test data: {mse}')\n",
    "\n",
    "# Example: Predict the risk score for each route and select the optimal one\n",
    "routes_risk_scores = []\n",
    "for route in routes:\n",
    "    route_features = generate_route_features(origin, destination, api_key)\n",
    "    route_risk_score = model.predict(route_features[['route_length', 'proximity_to_hotspot', 'route_duration']].values)\n",
    "    routes_risk_scores.append(route_risk_score)\n",
    "\n",
    "optimal_route = min(enumerate(routes_risk_scores), key=lambda x: x[1])  # Route with the lowest predicted risk score\n",
    "print(f\"Optimal Route: {optimal_route[0] + 1}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from geopy.distance import geodesic\n",
    "import json\n",
    "import requests\n",
    "import os\n",
    "from shapely.geometry import Point, Polygon\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load crash data\n",
    "filename = 'data.csv'\n",
    "data = pd.read_csv(filename, low_memory=False)\n",
    "data = data[['latitude', 'longitude', 'crash_sev_id', 'Crash timestamp (US/Central)']].dropna()\n",
    "\n",
    "# Preprocess crash data (normalize coordinates for clustering)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "coords_scaled = scaler.fit_transform(data[['latitude', 'longitude']].values)\n",
    "\n",
    "# Load the crash zones (GeoJSON) into a format we can work with\n",
    "output_dir = 'output_files'\n",
    "geojson_path = os.path.join(output_dir, 'high_crash_zones.geojson')\n",
    "with open(geojson_path, 'r') as f:\n",
    "    crash_zones = json.load(f)[\"features\"]\n",
    "\n",
    "# Function to calculate proximity to nearest crash hotspot\n",
    "def calculate_proximity_to_hotspot(route):\n",
    "    proximity_scores = []\n",
    "    \n",
    "    for leg in route['legs']:\n",
    "        for step in leg['steps']:\n",
    "            lat = step['end_location']['lat']\n",
    "            lng = step['end_location']['lng']\n",
    "            point = (lat, lng)\n",
    "            \n",
    "            # Calculate distance to each crash hotspot\n",
    "            min_distance = float('inf')\n",
    "            for zone in crash_zones:\n",
    "                zone_polygon = Polygon(zone['geometry']['coordinates'][0])\n",
    "                if zone_polygon.contains(Point(lng, lat)):\n",
    "                    distance = geodesic(point, (zone_polygon.centroid.y, zone_polygon.centroid.x)).km\n",
    "                    min_distance = min(min_distance, distance)\n",
    "            proximity_scores.append(min_distance)\n",
    "\n",
    "    return np.mean(proximity_scores)\n",
    "\n",
    "# Function to find nearest crash data point based on latitude and longitude\n",
    "def find_nearest_crash_severity(lat, lng, data, max_distance_km=1.0):\n",
    "    min_distance = float('inf')\n",
    "    crash_severity = None\n",
    "\n",
    "    for _, row in data.iterrows():\n",
    "        # Calculate the distance between route step and crash data point\n",
    "        crash_point = (row['latitude'], row['longitude'])\n",
    "        distance = geodesic((lat, lng), crash_point).km\n",
    "        \n",
    "        if distance < min_distance and distance <= max_distance_km:\n",
    "            min_distance = distance\n",
    "            crash_severity = row['crash_sev_id']\n",
    "    \n",
    "    # Return the closest severity found within the specified max distance (1 km default)\n",
    "    return crash_severity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 route options.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input X contains infinity or a value too large for dtype('float32').",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 54\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m# Initialize and train Random Forest Regressor\u001b[39;00m\n\u001b[1;32m     53\u001b[0m model \u001b[38;5;241m=\u001b[39m RandomForestRegressor(n_estimators\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m---> 54\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# Predict on the test set\u001b[39;00m\n\u001b[1;32m     57\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_test)\n",
      "File \u001b[0;32m~/Documents/GitHub/Smart-Drive/venv/lib/python3.11/site-packages/sklearn/base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1387\u001b[0m     )\n\u001b[1;32m   1388\u001b[0m ):\n\u001b[0;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/GitHub/Smart-Drive/venv/lib/python3.11/site-packages/sklearn/ensemble/_forest.py:375\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    369\u001b[0m \u001b[38;5;66;03m# _compute_missing_values_in_feature_mask checks if X has missing values and\u001b[39;00m\n\u001b[1;32m    370\u001b[0m \u001b[38;5;66;03m# will raise an error if the underlying tree base estimator can't handle missing\u001b[39;00m\n\u001b[1;32m    371\u001b[0m \u001b[38;5;66;03m# values. Only the criterion is required to determine if the tree supports\u001b[39;00m\n\u001b[1;32m    372\u001b[0m \u001b[38;5;66;03m# missing values.\u001b[39;00m\n\u001b[1;32m    373\u001b[0m estimator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimator)(criterion\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcriterion)\n\u001b[1;32m    374\u001b[0m missing_values_in_feature_mask \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 375\u001b[0m     \u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_compute_missing_values_in_feature_mask\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    376\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__class__\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\n\u001b[1;32m    377\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    378\u001b[0m )\n\u001b[1;32m    380\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    381\u001b[0m     sample_weight \u001b[38;5;241m=\u001b[39m _check_sample_weight(sample_weight, X)\n",
      "File \u001b[0;32m~/Documents/GitHub/Smart-Drive/venv/lib/python3.11/site-packages/sklearn/tree/_classes.py:222\u001b[0m, in \u001b[0;36mBaseDecisionTree._compute_missing_values_in_feature_mask\u001b[0;34m(self, X, estimator_name)\u001b[0m\n\u001b[1;32m    218\u001b[0m     overall_sum \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(X)\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39misfinite(overall_sum):\n\u001b[1;32m    221\u001b[0m     \u001b[38;5;66;03m# Raise a ValueError in case of the presence of an infinite element.\u001b[39;00m\n\u001b[0;32m--> 222\u001b[0m     \u001b[43m_assert_all_finite_element_wise\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcommon_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[38;5;66;03m# If the sum is not nan, then there are no missing values\u001b[39;00m\n\u001b[1;32m    225\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39misnan(overall_sum):\n",
      "File \u001b[0;32m~/Documents/GitHub/Smart-Drive/venv/lib/python3.11/site-packages/sklearn/utils/validation.py:169\u001b[0m, in \u001b[0;36m_assert_all_finite_element_wise\u001b[0;34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m estimator_name \u001b[38;5;129;01mand\u001b[39;00m input_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m has_nan_error:\n\u001b[1;32m    153\u001b[0m     \u001b[38;5;66;03m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;66;03m# scikit-learn.\u001b[39;00m\n\u001b[1;32m    155\u001b[0m     msg_err \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    156\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not accept missing values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    157\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    167\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#estimators-that-handle-nan-values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    168\u001b[0m     )\n\u001b[0;32m--> 169\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[0;31mValueError\u001b[0m: Input X contains infinity or a value too large for dtype('float32')."
     ]
    }
   ],
   "source": [
    "# Function to generate route features based on Google Maps data\n",
    "def generate_route_features(origin, destination, api_key):\n",
    "    routes = get_google_routes(origin, destination, api_key)\n",
    "    \n",
    "    data_points = []\n",
    "    \n",
    "    for route in routes:\n",
    "        # Calculate proximity to crash hotspots\n",
    "        proximity = calculate_proximity_to_hotspot(route)\n",
    "        \n",
    "        # Aggregate features (for simplicity, using only proximity here, but more can be added)\n",
    "        route_features = {\n",
    "            'route_length': sum([step['distance']['value'] for leg in route['legs'] for step in leg['steps']]),\n",
    "            'proximity_to_hotspot': proximity,\n",
    "            'route_duration': sum([step['duration']['value'] for leg in route['legs'] for step in leg['steps']])\n",
    "        }\n",
    "        \n",
    "        # Use crash severity in the route area as the target variable (this needs to be refined further)\n",
    "        crash_severity_list = []\n",
    "        for leg in route['legs']:\n",
    "            for step in leg['steps']:\n",
    "                lat = step['end_location']['lat']\n",
    "                lng = step['end_location']['lng']\n",
    "                crash_severity = find_nearest_crash_severity(lat, lng, data)\n",
    "                \n",
    "                if crash_severity is not None:\n",
    "                    crash_severity_list.append(crash_severity)\n",
    "        \n",
    "        if crash_severity_list:\n",
    "            route_features['average_crash_severity'] = np.mean(crash_severity_list)\n",
    "        else:\n",
    "            route_features['average_crash_severity'] = 0  # No severity data for this route\n",
    "        \n",
    "        data_points.append(route_features)\n",
    "        \n",
    "    return pd.DataFrame(data_points)\n",
    "\n",
    "# Example: Generate features and labels for training\n",
    "api_key = \"API_KEY\"\n",
    "origin = \"Austin, TX\"\n",
    "destination = \"Houston, TX\"\n",
    "\n",
    "route_data = generate_route_features(origin, destination, api_key)\n",
    "\n",
    "# Training the Model\n",
    "X = route_data[['route_length', 'proximity_to_hotspot', 'route_duration']]\n",
    "y = route_data['average_crash_severity']\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train Random Forest Regressor\n",
    "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f'Mean Squared Error on test data: {mse}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get Google Maps routes\n",
    "def get_google_routes(origin, destination, api_key):\n",
    "    base_url = \"https://maps.googleapis.com/maps/api/directions/json\"\n",
    "    params = {\n",
    "        \"origin\": origin,\n",
    "        \"destination\": destination,\n",
    "        \"mode\": \"driving\",\n",
    "        \"alternatives\": \"true\",  # Request alternative routes\n",
    "        \"key\": api_key\n",
    "    }\n",
    "    \n",
    "    response = requests.get(base_url, params=params)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        routes = response.json().get(\"routes\", [])\n",
    "        if routes:\n",
    "            print(f\"Found {len(routes)} route options.\")\n",
    "        return routes\n",
    "    else:\n",
    "        print(f\"Error: {response.status_code} - {response.text}\")\n",
    "        return []\n",
    "\n",
    "# Generate features and risk scores for routes\n",
    "routes_risk_scores = []\n",
    "for route in routes:\n",
    "    route_features = generate_route_features(origin, destination, api_key)\n",
    "    route_risk_score = model.predict(route_features[['route_length', 'proximity_to_hotspot', 'route_duration']].values)\n",
    "    routes_risk_scores.append(route_risk_score)\n",
    "\n",
    "# Identify optimal route (lowest risk score)\n",
    "optimal_route = min(enumerate(routes_risk_scores), key=lambda x: x[1])  # Route with the lowest predicted risk score\n",
    "print(f\"Optimal Route: {optimal_route[0] + 1}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
