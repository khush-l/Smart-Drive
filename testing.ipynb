{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import folium\n",
    "from folium.plugins import HeatMap\n",
    "import numpy as np\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from shapely.geometry import MultiPoint, Polygon\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Define output directory\n",
    "output_dir = 'output_files'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Load and preprocess the crash data\n",
    "filename = 'data.csv'\n",
    "data = pd.read_csv(filename, low_memory=False)\n",
    "\n",
    "data = data[['latitude', 'longitude', 'crash_sev_id', 'Crash timestamp (US/Central)']].dropna()\n",
    "\n",
    "scaler = StandardScaler()\n",
    "coords_scaled = scaler.fit_transform(data[['latitude', 'longitude']].values)\n",
    "\n",
    "# Train clustering model to find high-crash zones\n",
    "def train_cluster_model(data):\n",
    "    model = DBSCAN(eps=0.002, min_samples=20).fit(coords_scaled)\n",
    "    data['Cluster'] = model.labels_\n",
    "    return data, model\n",
    "\n",
    "clustered_data, model = train_cluster_model(data)\n",
    "\n",
    "clusters = clustered_data[clustered_data['Cluster'] != -1].groupby('Cluster')\n",
    "polygons = []\n",
    "\n",
    "for cluster_id, points in clusters:\n",
    "    cluster_points = points[['latitude', 'longitude']].drop_duplicates().values\n",
    "    if len(cluster_points) >= 4:  # Ensure valid polygon with at least 4 points\n",
    "        polygon = Polygon(MultiPoint(cluster_points).convex_hull)\n",
    "        polygons.append({\n",
    "            \"type\": \"Feature\",\n",
    "            \"geometry\": polygon.__geo_interface__,\n",
    "            \"properties\": {\"cluster_id\": int(cluster_id)}\n",
    "        })\n",
    "\n",
    "geojson_data = {\n",
    "    \"type\": \"FeatureCollection\",\n",
    "    \"features\": polygons\n",
    "}\n",
    "\n",
    "# Save high-crash zones as GeoJSON\n",
    "geojson_path = os.path.join(output_dir, 'high_crash_zones.geojson')\n",
    "with open(geojson_path, 'w') as f:\n",
    "    json.dump(geojson_data, f, indent=2)\n",
    "\n",
    "# Visualize crash data on a map\n",
    "austin_map = folium.Map(location=[30.2672, -97.7431], zoom_start=12)\n",
    "aggregated_crashes = data.groupby(['latitude', 'longitude']).agg({'crash_sev_id': 'sum'}).reset_index()\n",
    "max_severity = aggregated_crashes['crash_sev_id'].max()\n",
    "aggregated_crashes['size'] = aggregated_crashes['crash_sev_id'].apply(lambda x: (x / max_severity) * 20 + 5)\n",
    "\n",
    "for _, row in aggregated_crashes.iterrows():\n",
    "    folium.CircleMarker(\n",
    "        location=[row['latitude'], row['longitude']],\n",
    "        radius=row['size'],\n",
    "        color='red',\n",
    "        fill=True,\n",
    "        fill_color='red',\n",
    "        fill_opacity=0.6\n",
    "    ).add_to(austin_map)\n",
    "\n",
    "html_path = os.path.join(output_dir, \"austin_all_crashes_refined.html\")\n",
    "austin_map.save(html_path)\n",
    "\n",
    "# Save processed crash data as CSV\n",
    "csv_path = os.path.join(output_dir, 'processed_crash_data.csv')\n",
    "clustered_data.to_csv(csv_path, index=False)\n",
    "\n",
    "print(f\"Files saved in '{output_dir}':\")\n",
    "print(f\"- {csv_path}\")\n",
    "print(f\"- {geojson_path}\")\n",
    "print(f\"- {html_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
